{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43fed68f-03d0-47cb-bd10-45e25a1c16fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML files have been saved to the 'html_pages' directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "csv_file = 'pages.csv'  # Replace with your CSV file name\n",
    "html_column = 'page_content'  # Replace with the name of the column containing HTML code\n",
    "output_dir = 'html_pages'  # Directory to save the HTML files\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Iterate over the rows and save each HTML content to a file\n",
    "for index, row in df.iterrows():\n",
    "    html_content = row[html_column]  # Extract the HTML code\n",
    "    \n",
    "    # Handle missing or non-string values\n",
    "    if pd.isna(html_content):\n",
    "        html_content = \"\"  # Replace NaN or missing values with an empty string\n",
    "    else:\n",
    "        html_content = str(html_content)  # Ensure the content is a string\n",
    "    \n",
    "    # Create the output file name\n",
    "    output_file = os.path.join(output_dir, f'page_{index + 1}.html')\n",
    "    \n",
    "    # Write the HTML content to the file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "print(f\"HTML files have been saved to the '{output_dir}' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03db8e9e-2055-40f9-9440-85de2cadfdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text has been saved back to the original CSV file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the original CSV file\n",
    "csv_file = \"pages.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Function to extract text from HTML\n",
    "def extract_text_from_html(html_content):\n",
    "    if pd.isnull(html_content):  # Check for missing values\n",
    "        return \"\"  # Return empty string if content is null\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")  # Parse HTML\n",
    "    return soup.get_text(strip=True)  # Extract plain text\n",
    "\n",
    "# Apply the function to 'page_content' and overwrite or add a column\n",
    "df['cleaned_text'] = df['page_content'].apply(extract_text_from_html)\n",
    "\n",
    "# Save the updated DataFrame back to the original CSV file\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Cleaned text has been saved back to the original CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85440d97-b3d6-470b-895e-b1e1a50a613d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data has been saved to 'cleaned_reviews.csv' with title, link, description, and cleaned text.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the original CSV file\n",
    "input_csv = \"pages.csv\"  # Replace with your original CSV file path\n",
    "output_csv = \"cleaned_html.csv\"  # Path for the new CSV file\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Function to extract reviews/blogs from HTML content\n",
    "def extract_reviews_from_html(html_content):\n",
    "    if pd.isnull(html_content):  # Check for missing values\n",
    "        return \"\"  # Return empty string if content is null\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")  # Parse HTML\n",
    "    \n",
    "    # Remove unwanted tags like headings, navigation, and other noise\n",
    "    for tag in soup([\"h1\", \"h2\", \"h3\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "        tag.decompose()  # Remove the tag and its content\n",
    "    \n",
    "    # Extract desired content: paragraphs or relevant tags\n",
    "    reviews = []\n",
    "    for tag in soup.find_all([\"p\", \"article\", \"div\"]):\n",
    "        text = tag.get_text(strip=True)\n",
    "        if len(text) > 20:  # Skip short or irrelevant text\n",
    "            reviews.append(text)\n",
    "    \n",
    "    return \" \".join(reviews)  # Combine all extracted text into one string\n",
    "\n",
    "# Apply the function to clean 'page_content'\n",
    "df['cleaned_text'] = df['page_content'].apply(extract_reviews_from_html)\n",
    "\n",
    "# Select columns to include in the new CSV\n",
    "columns_to_save = ['Title', 'Link', 'Description', 'cleaned_text']\n",
    "df_cleaned = df[columns_to_save]  # Create a new DataFrame with selected columns\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "df_cleaned.to_csv(output_csv, index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Cleaned data has been saved to '{output_csv}' with title, link, description, and cleaned text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24abd94-f5d5-467b-9fb4-12bd2e845049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
