{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c869879c-9156-488e-9f35-f4fa25e7ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more items in response.\n",
      "Results saved to google_search_results_oman.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# Constants\n",
    "API_KEY = \"\"  # Replace with your Google API key\n",
    "CSE_ID = \"\"  # Replace with your Custom Search Engine ID\n",
    "SEARCH_QUERY = \"\"  # Replace with your search query\n",
    "NUM_RESULTS = 100  # Total number of results needed (max 100)\n",
    "\n",
    "def fetch_google_results(api_key, cse_id, query, num_results):\n",
    "    \"\"\"Fetches results from Google Custom Search JSON API.\"\"\"\n",
    "    results = []\n",
    "    start_index = 1\n",
    "    while len(results) < num_results:\n",
    "        # Ensure not to request more than the remaining results needed\n",
    "        max_results = min(num_results - len(results), 10)\n",
    "        \n",
    "        url = (\n",
    "            f\"https://www.googleapis.com/customsearch/v1\"\n",
    "            f\"?key={api_key}&cx={cse_id}&q={query}&start={start_index}\"\n",
    "        )\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        if \"items\" not in data:\n",
    "            print(\"No more items in response.\")\n",
    "            break\n",
    "\n",
    "        # Append results\n",
    "        for item in data[\"items\"]:\n",
    "            results.append({\n",
    "                \"Title\": item.get(\"title\"),\n",
    "                \"Link\": item.get(\"link\"),\n",
    "                \"Description\": item.get(\"snippet\"),\n",
    "            })\n",
    "        \n",
    "        # Update start index for the next batch (increment by 10)\n",
    "        start_index += max_results\n",
    "        \n",
    "        # Google Custom Search JSON API allows a maximum of 100 results\n",
    "        if start_index > 100:\n",
    "            print(\"Reached maximum retrievable results (100).\")\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_results_to_csv(results, filename):\n",
    "    \"\"\"Saves results to a CSV file.\"\"\"\n",
    "    fieldnames = [\"Title\", \"Link\", \"Description\"]\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "# Fetch results\n",
    "search_results = fetch_google_results(API_KEY, CSE_ID, SEARCH_QUERY, NUM_RESULTS)\n",
    "\n",
    "# Save to CSV\n",
    "if search_results:\n",
    "    save_results_to_csv(search_results, \"google_search_results_oman.csv\")\n",
    "else:\n",
    "    print(\"No results to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a03d9e-6b84-4069-9dab-93cf1bd43b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching content for: https://www.hyundaioman.com/en/\n",
      "Fetching content for: https://www.instagram.com/hyundaioman_official/?hl=en\n",
      "Fetching content for: https://www.hyundaioman.com/en/find-a-car/tucson-2021/highlights.php\n",
      "Fetching content for: https://oman.yallamotor.com/new-cars/hyundai\n",
      "Fetching content for: https://www.hyundaioman.com/en/find-a-car/sonata-2023/highlights.php\n",
      "Fetching content for: https://m.facebook.com/HyundaiOmanLive/?profile_tab_item_selected=mentions\n",
      "Fetching content for: https://oman.motoraty.com/m/motoraty-car-buying-guide/new-cars/hyundai\n",
      "Fetching content for: https://www.facebook.com/HyundaiOmanLive/\n",
      "Fetching content for: https://www.youtube.com/@hyundaioman3165\n",
      "Fetching content for: https://www.drivearabia.com/carprices/oman/hyundai/\n",
      "Fetching content for: https://www.dubizzle.com.om/en/vehicles/cars-for-sale/hyundai/muscat/\n",
      "Fetching content for: https://www.thearabianstories.com/2024/11/07/hyundai-oman-launches-exclusive-offer-with-8-iphone-prizes-and-comprehensive-ownership-benefits/\n",
      "Fetching content for: https://twitter.com/OmanHyundai/status/1443940205912748033\n",
      "Fetching content for: https://om.linkedin.com/in/malay-saha-05139b16\n",
      "Fetching content for: https://www.kavak.com/om-en/preowned/hyundai/oman\n",
      "Fetching content for: https://www.kavak.com/om-en/preowned/hyundai/oman-avenues-mall/suv\n",
      "Fetching content for: https://www.focus2move.com/the-oman-vehicles-market/\n",
      "Fetching content for: https://ar-ar.facebook.com/HyundaiOmanLive/photos\n",
      "Fetching content for: https://om.opensooq.com/en/cars/cars-for-sale/hyundai\n",
      "Fetching content for: https://www.otegroup.com/en/news/view/get-your-hyundai-serviced-in-just-one-hour\n",
      "Fetching content for: https://www.nationalcar.com/en/car-rental/vehicles/om/suvs/compact-suv-cfar.html\n",
      "Fetching content for: https://www.naukri.com/hyundai-jobs-in-oman\n",
      "Fetching content for: https://oman.hatla2ee.com/en/car/info/hyundai\n",
      "Fetching content for: https://www.linkedin.com/posts/mercedesbenzcarsoman_mercedesbenz-mercedesoman-gclass-activity-7256579886759198720-ltAb\n",
      "Fetching content for: https://apps.apple.com/us/app/hyundai-and-genesis-hq-events/id6618158328\n",
      "Fetching content for: https://www.pv-magazine.com/region/oman/page/2/\n",
      "Fetching content for: https://www.federalregister.gov/documents/2015/11/25/2015-30036/notice-of-agreements-filed\n",
      "Fetching content for: https://podcasts.apple.com/ke/podcast/unpacking-the-penrith-panthers-historic-4th-straight/id1629371037?i=1000671995979\n",
      "Fetching content for: http://walkingact.net/dummy-on-tour-video-hyundai-oman-2/\n",
      "Fetching content for: https://www.sciencedirect.com/science/article/pii/S0958211814702502\n",
      "Website content saved to oman_website_content.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def read_links_from_csv(filename):\n",
    "    \"\"\"Reads links from a CSV file.\"\"\"\n",
    "    links = []\n",
    "    with open(filename, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            links.append({\n",
    "                \"Title\": row[\"Title\"],\n",
    "                \"Link\": row[\"Link\"],\n",
    "                \"Description\": row[\"Description\"]\n",
    "            })\n",
    "    return links\n",
    "\n",
    "def extract_website_content(url):\n",
    "    \"\"\"Extracts text content from a website.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            return f\"Error fetching {url}: Status code {response.status_code}\"\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        # Extract meaningful text (ignore scripts, styles, etc.)\n",
    "        text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "        return text[:5000]  # Limit to 5000 characters to avoid overly large outputs\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching {url}: {e}\"\n",
    "\n",
    "def save_to_text_file(results, filename):\n",
    "    \"\"\"Saves extracted website content to a text file.\"\"\"\n",
    "    with open(filename, mode='w', encoding='utf-8') as file:\n",
    "        for result in results:\n",
    "            file.write(f\"Title: {result['Title']}\\n\")\n",
    "            file.write(f\"Link: {result['Link']}\\n\\n\")\n",
    "            file.write(\"Content:\\n\")\n",
    "            file.write(result['Content'])\n",
    "            file.write(\"\\n\\n---\\n\\n\")\n",
    "    print(f\"Website content saved to {filename}\")\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Read links from the CSV file\n",
    "    input_csv = \"google_search_results_oman.csv\"  # Replace with your CSV file name\n",
    "    links = read_links_from_csv(input_csv)\n",
    "\n",
    "    # Step 2: Crawl and extract content for each link\n",
    "    for link in links:\n",
    "        print(f\"Fetching content for: {link['Link']}\")\n",
    "        link['Content'] = extract_website_content(link['Link'])\n",
    "\n",
    "    # Step 3: Save to text file\n",
    "    output_text_file = \"oman_website_content.txt\"\n",
    "    save_to_text_file(links, output_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0770ec7-e3b6-40b7-b843-a542d0543904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
